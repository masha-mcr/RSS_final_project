# Финальный проект

* Использовала forest датасет
* poetry
* data в .gitignore

Получение EDА-отчета с pandas-profiling:

    poetry run eda_report -d data/train.csv

Пример отчета лежит в репозитории [profiling_report.html](https://github.com/masha-mcr/RSS_final_project/blob/master/profiling_report.html), но файл большой и вряд ли без скачивания его можно посмотреть.

Этот скрипт и train зарегистрированы в *pyproject.toml*.

## Обучение

Пример, как запустить обучение логистической регрессии без подбора гиперпараметров: 

    poetry run train -d data/train.csv -m log-reg --tuning default

* использовала метрики *accuracy*, *precision_weighted*, *recall_weigthed*. Еще бы попробовала f1 и ROC AUC, но времени было мало.
* что я делаю с гиперпараметрами: 
    1. Можно оставить дефолтные sklearn'овские параметры, тогда делю данные на train/test, на train делаю кросс-валидацию, выбираю из всех моделей ту, которая дала средний по accuracy результат, чтобы не завышать оценку. С помощью этой модели предсказываю значения из test, по ним считаю 3 метрики, они и идут в запись mlflow.
    Для этого варианта указывать `--tuning default` или не указывать вообще - это дефолтная опция.

    2. Можно оптимизировать гиперпараметры с RandomizedSearch, указывая `--tuning random`.
    Для поиска гиперпараметров используется NestedCV с 3 фолдами, из них выбираются гиперпараметры модели, показавшая лучший результат. Затем эта модель обучается на 5 внешних фолдах, из них выбирается модель, показавшая средний по accuracy результат, для нее считаются остальные метрики и заносятся в mlflow.

    3.  Можно оптимизировать гиперпараметры с GridSearch, указывая `--tuning grid`.
    Механизм тот же, что и для рандомного поиска. 
    **ВАЖНО** На моей машине очень долго считает. Также иногда вылезают предепреждения типа FitFailed или ConvergenceWarning (особенно, для логистической регрессии) - они не влияют на работу, но очень засоряют вывод. Я пробовала прописать игнорирование предупреждений, но ни один способ не помог( 
    Долго и некрасиво, но оно работает, честное слово. 

* доступные алгоритмы: указывать `-m log-reg` для логистической регрессии, `-m tree` для решающего дерева и `-m forest` для случайного леса. Параметр является обязательным.


